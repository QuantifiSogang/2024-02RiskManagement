{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d94387d",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf2152",
   "metadata": {},
   "source": [
    "### Synthetic data from real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68208f",
   "metadata": {},
   "source": [
    "합성 데이터 생성은 데이터 요구 사항 증가와 데이터에 대한 프라이버시 문제 등으로 인해 주목받고 있는 분야이다. 은닉 마르코프 모델(HMM)은 합성 데이터를 생성하는 대표적인 모델 중 하나이다. \n",
    "\n",
    "합성 데이터는 실제 데이터의 통계적 특성을 모방하는 과정에서 생성된 데이터이다. 데이터는 원 데이터로부터 모델링되어야 한다는 믿음이 있지만, 반드시 실제 데이터에서 합성 데이터를 생성하는 것 만이 유일한 방법은 아니다. 합성 데이터를 생성하는 방법에는 세 가지 방법이 알려져 있다.\n",
    "\n",
    "1. 실제 데이터를 이용해 합성 데이터를 생성하는 방법. 이 프로세스는 실제 데이터를 가져오는 것에서 시작해 데이터 분포를 모델링하고 마지막으로 해당 분포 모델로부터 합성 데이터를 샘플링한다.\n",
    "\n",
    "2. 합성 데이터는 모델 또는 지식으로부터 얻을 수 있다. 일반적으로 이러한 유형의 합성 데이터 생성은 기존 모델을 사용하거나 연구자의 지식을 바탕으로 생성된다.\n",
    "\n",
    "3. 하이브리드 프로세스는 앞의 두 단계를 모두 포함한다. 이런 하이브리드 프로세스는 데이터의 일부만 사용 가능할 때 많이 적용되며, 실제 데이터로 합성 데이터를 일부 생성한 뒤 나머지 부분을 모델로부터 얻어온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3201912",
   "metadata": {},
   "source": [
    "그렇다면 합성 데이터의 품질은 어떻게 평가할 수 있을까? 일반적으로는 KL-분산, 구별 가능성(Distinguishable), ROC 곡선, 평균&중앙값 4가지 방법론이 사용된다. \n",
    "\n",
    "구별 가능성(Distinguishable)은 분류 모델이 실제 데이터와 합성 데이터를 구분하는 경우 실제 데이터에 1을 할당한다. 만일 실제 데이터와 합성 데이터를 구분하지 못한다면 0을 할당한다. 출력이 1에 가까우면 데이터는 실제라고 예측하고, 그렇지 않으면 성향 점수(Propensity score)를 사용해 합성 데이터라고 예측한다.\n",
    "\n",
    "혹은 실제 데이터와 합성 데이터의 평균과 분산과 같이 주요 통계량을 이용해 합성 데이터가 실제 데이터를 잘 모방하는지 파악할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5b588",
   "metadata": {},
   "source": [
    "다음은 실제 데이터를 이용해 합성 데이터를 만들어 내는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15547c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib. pyplot as plt\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4372d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89d8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing=np.column_stack([X, y])\n",
    "california_housing_df=pd.DataFrame(california_housing)\n",
    "california_housing_df=california_housing_df.iloc[:15000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954e5555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       15000 non-null  float64\n",
      " 1   1       15000 non-null  float64\n",
      " 2   2       15000 non-null  float64\n",
      " 3   3       15000 non-null  float64\n",
      " 4   4       15000 non-null  float64\n",
      " 5   5       15000 non-null  float64\n",
      " 6   6       15000 non-null  float64\n",
      " 7   7       15000 non-null  float64\n",
      " 8   8       15000 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "california_housing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bb106",
   "metadata": {},
   "source": [
    "### 생성적 적대 네트워크(GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c080a73",
   "metadata": {},
   "source": [
    "생성적 적대 네트워크(이하 GAN)는 두가지 모델로 이뤄져 있다. 하나는 데이터를 합성하는 모델이며, 하나는 입력받은 데이터가 합성된 데이터인지, 아니면 실제 데이터인지 판별하는 모델이다. GAN은 데이터를 합성한 뒤 (혹은 그냥 진짜 데이터를) 판별 모델이 받아 실제인지 합성인지 판별하고 이를 다시 합성 모델에 전달하는 식으로 훈련이 진행된다. 현금 위조범과 위조 판별사를 생각해보자. 현금 위조범의 첫 결과물은 너무 조잡하여 위조 판별사는 간단하게 실제 현금과 위조 현금을 구분할 수 있을 것이다. 하지만 이 결과를 위조범이 계속 받게 되어 시간이 지날수록 더욱 정교한 위조 현금이 생성될 것이다. 결과적으로, 위조 판별사는 실제 현금과 위조 현금을 분별하지 못할 것이다.\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./Image/GAN.png\" width=\"700px\" height=\"400px\">\n",
    "    \n",
    "</center>\n",
    "\n",
    "이를 수식적으로 서술하면 다음과 같다.\n",
    "\n",
    "$$ \\min_G \\max_D V(D, G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))]$$\n",
    "\n",
    "여기서 G는 생성 모델을 의미하며, D는 판별 모델을 의미한다. 생성 모델은 데이터 x로부터 생성 분포 $p_g$를 학습하고자 한다. 따라서 생성 모델은 사전 잡음 분포 $p_z(z)$로부터 데이터 스페이스 $G(z;\\theta_g)$ 로 매핑하는 함수를 만든다. 여기서 Z는 Latent vector라고도 하며 데이터의 분포를 잘 설명하는 잠재 공간에서의 벡터를 의미한다. 판별 모델 $D(x;\\theta_d)$는 x가 생성 분포 $p_g$가 아닌 데이터 x에서 올 확률을 계산한다. 앞쪽 파트 $E_{x\\sim p_{data}(x)}[logD(x)]$는 판별 모델로 온 데이터가 실제 데이터에서 왔을 것으로 판단되는 기대값을 의미하며, $E_{z\\sim p_{z}(z)}[log(1-D(G(z)))]$는 데이터가 생성 모델로부터 온 것으로 판단하는 기대값을 의미한다. \n",
    "\n",
    "분류모델은 해당 손실함수를 최대화해야 하고 생성모델은 손실함수를 최소화 해야 한다. 그러기 위해서는 분류 모델의 입장에서는 $D(x)=1, \\ D(G(z))=0$이 되어야 한다. 즉 생성모델이 만들어 낸 데이터는 가짜로 분류해야 하고, 진짜 데이터는 진짜로 판별해야 한다. 생성 모델의 입장에서는 $D(G(z))=1$로 만들어야 한다. 즉, 분류모델이 진짜라고 판단할 수준의 데이터를 만들어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5560197",
   "metadata": {},
   "source": [
    "### 조건부 GAN(Conditional Generative Adversarial Nets)\n",
    "\n",
    "조건부 GAN(이하 CGAN)은 GAN과 비슷하나 약간의 차이점을 가진다. 생성 모델과 판별 모델이 새로운 정보 y에 근거하여 만들어져야 한다는 것이다. 여기서 y는 class label, 다른 양식의 데이터 등 어떠한 보조적 데이터든 될 수 있다. 이 사전 정보는 추가적인 input layer로 생성 모델과 판별 모델 모두에게 주어진다.\n",
    "\n",
    "수식적으로 설명하면 다음과 같다. 생성모델의 사전 input noize(혹은 Latent vector) $p_z(z)$와 y는 joint hidden representation으로 결합된다. 그리고 적대적 훈련 기법은 joint hidden representation을 구현하는 과정에 충분한 유동성을 제공한다.\n",
    "\n",
    "CGAN의 목적함수는 다음과 같이 변경된다.\n",
    "\n",
    "$$ \\min_G \\max_D V(D, G) = E_{x\\sim p_{data}(x)}[logD(x\\vert y)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z\\vert y)))]$$\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./Image/CGAN.png\" width=\"700px\" height=\"400px\">\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3061de",
   "metadata": {},
   "source": [
    "`ctgan` 라이브러리는 생성적 적대 네트워크(GAN)모델을 이용해 원본 데이터에 대한 충실도가 높은 합성 데이터를 생성해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90609726",
   "metadata": {},
   "outputs": [],
   "source": [
    "column=[str(i) for i in range(9)]\n",
    "california_housing_df.columns=column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be1ef00",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 13663797636 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mctgan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CTGAN\n\u001b[0;32m      3\u001b[0m ctgan \u001b[38;5;241m=\u001b[39m CTGAN(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m ctgan\u001b[38;5;241m.\u001b[39mfit(california_housing_df, california_housing_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m synt_sample \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlen\u001b[39m(california_housing_df))\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\ctgan\\synthesizers\\base.py:50\u001b[0m, in \u001b[0;36mrandom_state.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m set_random_states(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_random_state):\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\ctgan\\synthesizers\\ctgan.py:439\u001b[0m, in \u001b[0;36mCTGAN.fit\u001b[1;34m(self, train_data, discrete_columns, epochs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     optimizerG\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m     loss_g\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 439\u001b[0m     optimizerG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    441\u001b[0m generator_loss \u001b[38;5;241m=\u001b[39m loss_g\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    442\u001b[0m discriminator_loss \u001b[38;5;241m=\u001b[39m loss_d\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kumb2\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 13663797636 bytes."
     ]
    }
   ],
   "source": [
    "from ctgan import CTGAN\n",
    "\n",
    "ctgan = CTGAN(epochs=10)\n",
    "ctgan.fit(california_housing_df, california_housing_df.columns)\n",
    "synt_sample = ctgan.sample(len(california_housing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d195987",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "synt_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abe89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation import evaluate\n",
    "\n",
    "evaluate(synt_sample, california_housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed12311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import TableEvaluator\n",
    "\n",
    "table_evaluator =  TableEvaluator(california_housing_df, synt_sample)\n",
    "\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6abe40",
   "metadata": {},
   "source": [
    "### Synthetic data from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17caa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=3, noise=0.2,\n",
    "                       random_state=123)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha= 0.3, cmap='Greys', c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "k = 0\n",
    "\n",
    "for i in range(0, 10):\n",
    "    X, y = make_regression(n_samples=100, n_features=3, noise=i,\n",
    "                           random_state=123) \n",
    "    k+=1\n",
    "    plt.subplot(5, 2, k)\n",
    "    profit_margin_orange = np.asarray([20, 35, 40])\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.3, cmap=cm.Greys, c=y)\n",
    "    plt.title('Synthetic Data with Different Noises: ' + str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d249dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "k = 0\n",
    "\n",
    "for i in range(2, 6):\n",
    "    X, y = make_classification(n_samples=100,\n",
    "                               n_features=4,\n",
    "                               n_classes=i,\n",
    "                               n_redundant=0,\n",
    "                               n_informative=4,\n",
    "                               random_state=123)\n",
    "    k+=1\n",
    "    plt.subplot(2, 2, k)\n",
    "    plt.scatter(X[: ,0], X[:, 1], alpha=0.8, cmap='gray', c=y)\n",
    "    plt.title('Synthetic Data with Different Classes: ' + str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b543a0",
   "metadata": {},
   "source": [
    "## Synthetic Data for Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7601cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ab91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2, \n",
    "                      n_features=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ec07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "k = 0\n",
    "for i in range(2, 6):\n",
    "    X, y = make_blobs(n_samples=100, centers=i,\n",
    "                      n_features=2, random_state=0)\n",
    "    k += 1\n",
    "    plt.subplot(2, 2, k)\n",
    "    my_scatter_plot = plt.scatter(X[:, 0], X[:, 1],\n",
    "                                  alpha=0.3, cmap='gray', c=y)\n",
    "    plt.title('Synthetic Data with Different Clusters: ' + str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16258b7a",
   "metadata": {},
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.read_csv('datasets/FF3.csv', skiprows=4)\n",
    "ff = ff.rename(columns={'Unnamed: 0': 'Date'})\n",
    "ff = ff.iloc[:-1]\n",
    "ff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39442b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff['Date'] = pd.to_datetime(ff['Date'])\n",
    "ff.set_index('Date', inplace=True)\n",
    "ff_trim = ff.loc['2000-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51699036",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_trim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'SPY'\n",
    "start = datetime.datetime(2000, 1, 3)\n",
    "end = datetime.datetime(2021, 4, 30)\n",
    "SP_ETF = yf.download(ticker, start, end, interval='1d').Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4080d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_merge = pd.merge(ff_trim, SP_ETF, how='inner', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP = pd.DataFrame()\n",
    "SP['Close']= ff_merge['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfb00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SP['return'] = (SP['Close'] / SP['Close'].shift(1))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7760a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = hmm.GaussianHMM(n_components=3,\n",
    "                            covariance_type=\"full\",\n",
    "                            n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model.fit(np.array(SP['return'].dropna()).reshape(-1, 1))\n",
    "hmm_predict = hmm_model.predict(np.array(SP['return'].dropna())\n",
    "                                .reshape(-1, 1))\n",
    "df_hmm = pd.DataFrame(hmm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_merged = pd.concat([df_hmm,SP['return'].dropna().reset_index()],\n",
    "                       axis=1)\n",
    "ret_merged.drop('Date',axis=1, inplace=True)\n",
    "ret_merged.rename(columns={0:'states'}, inplace=True)\n",
    "ret_merged.dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c16930",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_merged['states'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_means = []\n",
    "state_std = []\n",
    "\n",
    "for i in range(3):\n",
    "    state_means.append(ret_merged[ret_merged.states == i]['return']\n",
    "                       .mean())\n",
    "    state_std.append(ret_merged[ret_merged.states == i]['return']\n",
    "                     .std())\n",
    "print('State Means are: {}'.format(state_means))\n",
    "print('State Standard Deviations are: {}'.format(state_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd141016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'HMM means\\n {hmm_model.means_}')\n",
    "print(f'HMM covariances\\n {hmm_model.covars_}')\n",
    "print(f'HMM transition matrix\\n {hmm_model.transmat_}')\n",
    "print(f'HMM initial probability\\n {hmm_model.startprob_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3adf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ret = SP['return'].dropna().values.reshape(-1,1)\n",
    "n_components = np.arange(1, 10)\n",
    "clusters = [hmm.GaussianHMM(n_components=n, \n",
    "                            covariance_type=\"full\").fit(sp_ret)\n",
    "           for n in n_components]\n",
    "plt.plot(n_components, [m.score(np.array(SP['return'].dropna())\\\n",
    "                                .reshape(-1,1)) for m in clusters])\n",
    "plt.title('Optimum Number of States')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Log Likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = hmm.GaussianHMM(n_components=3, \n",
    "                        covariance_type=\"full\", \n",
    "                        random_state=123).fit(sp_ret)\n",
    "hidden_states = hmm_model.predict(sp_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d276d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import YearLocator, MonthLocator\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64648060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_ret = SP['return'].dropna()\n",
    "\n",
    "hmm_model = hmm.GaussianHMM(n_components=3, \n",
    "                            covariance_type=\"full\", \n",
    "                            random_state=123).fit(sp_ret)\n",
    "\n",
    "hidden_states = hmm_model.predict(sp_ret)\n",
    "\n",
    "fig, axs = plt.subplots(hmm_model.n_components, sharex=True,\n",
    "                        sharey=True, figsize=(12, 9))\n",
    "colors = cm.gray(np.linspace(0, 0.7, hmm_model.n_components))\n",
    "\n",
    "for i, (ax, color) in enumerate(zip(axs, colors)):\n",
    "    mask = hidden_states == i\n",
    "    ax.plot_date(df_sp_ret.index.values[mask],\n",
    "                 df_sp_ret.values[mask],\n",
    "                 \".-\", c=color)\n",
    "    ax.set_title(\"Hidden state {}\".format(i + 1), fontsize=16)\n",
    "    ax.xaxis.set_minor_locator(MonthLocator())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3bc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_merged.groupby('states')['return'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b6180",
   "metadata": {},
   "source": [
    "## Fama-French Model vs. HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3574be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_merge['return'] = ff_merge['Close'].pct_change()\n",
    "ff_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b35ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(ff_merge) * 0.9)\n",
    "train_ff= ff_merge.iloc[:split].dropna()\n",
    "test_ff = ff_merge.iloc[split:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = hmm.GaussianHMM(n_components=3,\n",
    "                            covariance_type=\"full\",\n",
    "                            n_iter=100, init_params=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(len(test_ff)):\n",
    "    hmm_model.fit(train_ff)\n",
    "    adjustment = np.dot(hmm_model.transmat_, hmm_model.means_)\n",
    "    predictions.append(test_ff.iloc[i] + adjustment[0])\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0df7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = predictions['return'].std()\n",
    "sharpe = predictions['return'].mean() / std_dev\n",
    "print('Sharpe ratio with HMM is {:.4f}'.format(sharpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82993b8a",
   "metadata": {},
   "source": [
    "## Fama-French Model with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c57902",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_ff['return']\n",
    "X = train_ff[['Mkt-RF', 'SMB', 'HML']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcec34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(Y, X)\n",
    "ff_ols = model.fit()\n",
    "print(ff_ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_pred = ff_ols.predict(test_ff[[\"Mkt-RF\", \"SMB\", \"HML\"]])\n",
    "ff_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e91c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = ff_pred.std()\n",
    "sharpe = ff_pred.mean() / std_dev\n",
    "print('Sharpe ratio with FF 3 factor model is {:.4f}'.format(sharpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(SP['return']) * 0.9)\n",
    "train_ret_SP = SP['return'].iloc[split:].dropna()\n",
    "test_ret_SP = SP['return'].iloc[:split].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc83e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = hmm.GaussianHMM(n_components=3,\n",
    "                            covariance_type=\"full\",\n",
    "                            n_iter=100)\n",
    "hmm_model.fit(np.array(train_ret_SP).reshape(-1, 1))\n",
    "hmm_predict_vol = hmm_model.predict(np.array(test_ret_SP)\n",
    "                                    .reshape(-1, 1))\n",
    "pd.DataFrame(hmm_predict_vol).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a110d6",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation and Hidden Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aed2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "startprob = hmm_model.startprob_\n",
    "transmat = hmm_model.transmat_\n",
    "means = hmm_model.means_ \n",
    "covars = hmm_model.covars_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f550c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_hmm = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_hmm.startprob_ = startprob\n",
    "syn_hmm.transmat_ = transmat \n",
    "syn_hmm.means_ = means \n",
    "syn_hmm.covars_ = covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7992d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data, _ = syn_hmm.sample(n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ca78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(syn_data)\n",
    "plt.title('Histogram of Synthetic Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(syn_data, \"--\")\n",
    "plt.title('Line Plot of Synthetic Data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
