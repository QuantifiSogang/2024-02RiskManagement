{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data = pd.read_csv('datasets/crash_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data.date = pd.to_datetime(crash_data.date, format='%Y%m%d')\n",
    "crash_data = crash_data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_dataw = crash_data.groupby('TICKER').resample('W').\\\n",
    "              agg({'RET':'mean', 'vwretx':'mean', 'VOL':'mean',\n",
    "                   'BIDLO':'mean', 'ASKHI':'mean', 'PRC':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_dataw = crash_dataw.reset_index()\n",
    "crash_dataw.dropna(inplace=True)\n",
    "stocks = crash_dataw.TICKER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "k = 1\n",
    "\n",
    "for i in stocks[: 4]:\n",
    "    plt.subplot(2, 2, k)\n",
    "    plt.hist(crash_dataw[crash_dataw.TICKER == i]['RET'])\n",
    "    plt.title('Histogram of '+i)\n",
    "    k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm-specific return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "residuals = []\n",
    "\n",
    "for i in stocks:\n",
    "    Y = crash_dataw.loc[crash_dataw['TICKER'] == i]['RET'].values\n",
    "    X = crash_dataw.loc[crash_dataw['TICKER'] == i]['vwretx'].values\n",
    "    X = sm.add_constant(X)\n",
    "    ols = sm.OLS(Y[2:-2], X[2:-2] + X[1:-3] + X[0:-4] + \\\n",
    "                 X[3:-1] + X[4:]).fit()\n",
    "    residuals.append(ols.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = list(map(lambda x: np.log(1 + x), residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data_sliced = pd.DataFrame([])\n",
    "for i in stocks:\n",
    "    crash_data_sliced = crash_data_sliced.\\\n",
    "                        append(crash_dataw.loc[crash_dataw.TICKER == i]\n",
    "                               [2:-2])\n",
    "crash_data_sliced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elliptic Envelope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "envelope = EllipticEnvelope(contamination=0.02, support_fraction=1)\n",
    "ee_predictions = {}\n",
    "\n",
    "for i, j in zip(range(len(stocks)), stocks):\n",
    "    envelope.fit(np.array(residuals[i]).reshape(-1, 1))\n",
    "    ee_predictions[j] = envelope.predict(np.array(residuals[i])\n",
    "                                         .reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = []\n",
    "\n",
    "for i in stocks:\n",
    "    for j in range(len(ee_predictions[i])):\n",
    "        transform.append(np.where(ee_predictions[i][j] == 1, 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data_sliced = crash_data_sliced.reset_index()\n",
    "crash_data_sliced['residuals'] = np.concatenate(residuals)\n",
    "crash_data_sliced['neg_outliers'] = np.where((np.array(transform)) == -1, 1, 0)\n",
    "crash_data_sliced.loc[(crash_data_sliced.neg_outliers == 1) &\n",
    "                      (crash_data_sliced.residuals > 0),\n",
    "                      'neg_outliers'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data_sliced['neg_outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8)) \n",
    "k=1\n",
    "\n",
    "for i in stocks[8:12]:\n",
    "    plt.subplot(2, 2, k)\n",
    "    crash_data_sliced['residuals'][crash_data_sliced.TICKER == i]\\\n",
    "    .hist(label='normal', bins=30, color='gray')\n",
    "    outliers = crash_data_sliced['residuals'][(crash_data_sliced.TICKER == i) &\n",
    "    (crash_data_sliced.neg_outliers > 0)]\n",
    "    outliers.hist(color='black', label='anomaly') \n",
    "    plt.title(i)\n",
    "    plt.legend()\n",
    "    k+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_data_sliced = crash_data_sliced.set_index('date')\n",
    "crash_data_sliced.index = pd.to_datetime(crash_data_sliced.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = crash_data.groupby('TICKER')['RET'].resample('W').std()\\\n",
    "      .reset_index()\n",
    "crash_dataw['std'] = pd.DataFrame(std['RET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data = crash_data_sliced.groupby('TICKER')['residuals']\\\n",
    "              .resample('Y').agg({'residuals':{'mean', 'std'}})\\\n",
    "              .reset_index()\n",
    "yearly_data.columns = ['TICKER', 'date', 'mean', 'std']\n",
    "yearly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash = pd.merge(crash_data_sliced.reset_index(), yearly_data,\n",
    "                       how='outer', on=['TICKER', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash[['annual_mean', 'annual_std']] = merge_crash\\\n",
    "                                             .sort_values(by=['TICKER',\n",
    "                                                              'date'])\\\n",
    "                                             .iloc[:, -2:]\\\n",
    "                                             .fillna(method='bfill')\n",
    "merge_crash['residuals'] = merge_crash.sort_values(by=['TICKER',\n",
    "                                                       'date'])\\\n",
    "                                                      ['residuals']\\\n",
    "                                             .fillna(method='ffill')\n",
    "merge_crash = merge_crash.drop(merge_crash.iloc[: ,-4:-2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_risk_out = []\n",
    "\n",
    "for j in stocks:\n",
    "    for k in range(len(merge_crash[merge_crash.TICKER == j])):\n",
    "        if merge_crash[merge_crash.TICKER == j]['residuals'].iloc[k] < \\\n",
    "        merge_crash[merge_crash.TICKER == j]['annual_mean'].iloc[k] - \\\n",
    "        3.09 * \\\n",
    "        merge_crash[merge_crash.TICKER == j]['annual_std'].iloc[k]:\n",
    "            crash_risk_out.append(1)\n",
    "        else:\n",
    "            crash_risk_out.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash['crash_risk'] = crash_risk_out\n",
    "merge_crash['crash_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash = merge_crash.set_index('date')\n",
    "merge_crash_annual = merge_crash.groupby('TICKER')\\\n",
    "                     .resample('1Y')['crash_risk'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down = []\n",
    "\n",
    "for j in range(len(merge_crash)):\n",
    "    if merge_crash['residuals'].iloc[j] < \\\n",
    "       merge_crash['annual_mean'].iloc[j]:\n",
    "        down.append(1)\n",
    "    else:\n",
    "        down.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash = merge_crash.reset_index()\n",
    "merge_crash['down'] = pd.DataFrame(down)\n",
    "merge_crash['up'] = 1 - merge_crash['down']\n",
    "down_residuals = merge_crash[merge_crash.down == 1]\\\n",
    "                 [['residuals', 'TICKER', 'date']]\n",
    "up_residuals = merge_crash[merge_crash.up == 1]\\\n",
    "               [['residuals', 'TICKER', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_residuals['residuals_down_sq'] = down_residuals['residuals'] ** 2\n",
    "down_residuals['residuals_down_cubic'] = down_residuals['residuals'] **3\n",
    "up_residuals['residuals_up_sq'] = up_residuals['residuals'] ** 2\n",
    "up_residuals['residuals_up_cubic'] = up_residuals['residuals'] ** 3\n",
    "down_residuals['down_residuals'] = down_residuals['residuals']\n",
    "up_residuals['up_residuals'] = up_residuals['residuals']\n",
    "del down_residuals['residuals']\n",
    "del up_residuals['residuals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash['residuals_sq'] = merge_crash['residuals'] ** 2\n",
    "merge_crash['residuals_cubic'] = merge_crash['residuals'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash_all = merge_crash.merge(down_residuals,\n",
    "                                    on=['TICKER', 'date'],\n",
    "                                    how='outer')\n",
    "merge_crash_all = merge_crash_all.merge(up_residuals,\n",
    "                                        on=['TICKER', 'date'],\n",
    "                                        how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BIDLO', 'ASKHI', 'residuals', \n",
    "        'annual_std', 'residuals_sq', 'residuals_cubic',\n",
    "        'down', 'up', 'residuals_up_sq', 'residuals_down_sq',\n",
    "        'neg_outliers']\n",
    "merge_crash_all = merge_crash_all.set_index('date')\n",
    "merge_grouped = merge_crash_all.groupby('TICKER')[cols]\\\n",
    "                .resample('1Y').sum().reset_index()\n",
    "merge_grouped['neg_outliers'] = np.where(merge_grouped.neg_outliers >=\n",
    "                                         1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped = merge_grouped.set_index('date')\n",
    "merge_all = merge_grouped.groupby('TICKER')\\\n",
    "            .resample('1Y').agg({'down':['sum', 'count'],\n",
    "                                 'up':['sum', 'count']})\\\n",
    "            .reset_index()\n",
    "merge_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['down'] = merge_all['down']['sum'].values\n",
    "merge_grouped['up'] = merge_all['up']['sum'].values\n",
    "merge_grouped['count'] = merge_grouped['down'] + merge_grouped['up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped = merge_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['duvol'] = np.log(((merge_grouped['up'] - 1) * \n",
    "                                 merge_grouped['residuals_down_sq']) /\n",
    "                                ((merge_grouped['down'] - 1) * \n",
    "                                 merge_grouped['residuals_up_sq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['duvol'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['ncskew'] = - (((merge_grouped['count'] * \n",
    "                               (merge_grouped['count'] - 1) **\n",
    "                               (3 / 2)) * \n",
    "                             merge_grouped['residuals_cubic']) / \n",
    "                             (((merge_grouped['count'] - 1) * \n",
    "                               (merge_grouped['count'] - 2)) * \n",
    "                              merge_grouped['residuals_sq'] **\n",
    "                              (3 / 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['ncskew'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_grouped['crash_risk'] = merge_crash_annual['crash_risk']\n",
    "merge_grouped['crash_risk'] = np.where(merge_grouped.crash_risk >= \n",
    "                                       1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_crash_all_grouped2 = merge_crash_all.groupby('TICKER')\\\n",
    "                            [['VOL', 'PRC']]\\\n",
    "                           .resample('1Y').mean().reset_index()\n",
    "merge_grouped[['VOL', 'PRC']] = merge_crash_all_grouped2[['VOL', 'PRC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped[['ncskew','duvol']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Sheet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = pd.read_csv('datasets/bs_v.3.csv')\n",
    "bs['Date'] = pd.to_datetime(bs.datadate, format='%Y%m%d')\n",
    "bs['annual_date'] = bs['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs['RoA'] = bs['ni'] / bs['at']\n",
    "bs['leverage'] = bs['lt'] / bs['at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_grouped['annual_date'] = merge_grouped['date'].dt.year\n",
    "bs['TICKER'] = bs.tic\n",
    "del bs['tic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs = pd.merge(bs, merge_grouped,\n",
    "                        on=['TICKER', 'annual_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2 = merge_ret_bs.set_index('Date')\n",
    "merge_ret_bs2 = merge_ret_bs2.groupby('TICKER').resample('Y').mean()\n",
    "merge_ret_bs2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2['vol_csho_diff'] = (merge_ret_bs2.groupby('TICKER')\n",
    "                                  ['VOL'].shift(-1) / \n",
    "                                  merge_ret_bs2.groupby('TICKER')\n",
    "                                  ['csho'].shift(-1))\n",
    "merge_ret_bs2['dturn1'] = merge_ret_bs2['VOL'] / merge_ret_bs2['csho']\n",
    "merge_ret_bs2['dturn'] = merge_ret_bs2['vol_csho_diff'] - \\\n",
    "                         merge_ret_bs2['dturn1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2['p/e'] = merge_ret_bs2['PRC'] / merge_ret_bs2['ni']\n",
    "merge_ret_bs2['turnover_rate'] = merge_ret_bs2['VOL'] / \\\n",
    "                                 merge_ret_bs2['csho']\n",
    "merge_ret_bs2['equity_share'] = merge_ret_bs2['ceq'] / \\\n",
    "                                (merge_ret_bs2['ceq'] +\n",
    "                                 merge_ret_bs2['dt'])\n",
    "merge_ret_bs2['firm_size'] = np.log(merge_ret_bs2['at'])\n",
    "merge_ret_bs2['cefd'] = (((merge_ret_bs2['at'] -\n",
    "                           merge_ret_bs2['lt']) / merge_ret_bs2['csho']) - \n",
    "                           merge_ret_bs2['PRC']) / (merge_ret_bs2['at'] - \n",
    "                           merge_ret_bs2['lt']) / merge_ret_bs2['csho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2 = merge_ret_bs2.set_index('Date')\n",
    "merge_ret_bs2['buying_volume'] = merge_ret_bs2['VOL'] * \\\n",
    "                                 (merge_ret_bs2['PRC'] - \n",
    "                                  merge_ret_bs2['BIDLO']) / \\\n",
    "                                 (merge_ret_bs2['ASKHI'] - \n",
    "                                  merge_ret_bs2['BIDLO'])\n",
    "merge_ret_bs2['selling_volume'] = merge_ret_bs2['VOL'] * \\\n",
    "                                  (merge_ret_bs2['ASKHI'] - \n",
    "                                   merge_ret_bs2['PRC']) / \\\n",
    "                                  (merge_ret_bs2['ASKHI'] - \n",
    "                                   merge_ret_bs2['BIDLO'])\n",
    "buying_volume = merge_ret_bs2.groupby('TICKER')['buying_volume'] \\\n",
    "                .resample('Y').sum().reset_index()\n",
    "selling_volume = merge_ret_bs2.groupby('TICKER')['selling_volume'] \\\n",
    "                .resample('Y').sum().reset_index()\n",
    "del buying_volume['TICKER']\n",
    "del buying_volume['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_sel_vol = pd.concat([buying_volume,selling_volume], axis=1)\n",
    "buy_sel_vol['bsi'] = (buy_sel_vol.buying_volume - \n",
    "                      buy_sel_vol.selling_volume) / \\\n",
    "                     (buy_sel_vol.buying_volume + \n",
    "                      buy_sel_vol.selling_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2 = merge_ret_bs2.reset_index()\n",
    "merge_ret_bs2 = pd.merge(buy_sel_vol ,merge_ret_bs2,\n",
    "                         on=['TICKER', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm Sentiment via PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_sentiment = merge_ret_bs2[['p/e', 'turnover_rate',\n",
    "                                'equity_share', 'cefd',\n",
    "                                'leverage', 'bsi']]\n",
    "firm_sentiment = firm_sentiment.apply(lambda x: x.fillna(x.mean()),\n",
    "                                      axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firm_sentiment_std = StandardScaler().fit_transform(firm_sentiment)\n",
    "pca = PCA(n_components=6)\n",
    "pca_market_sentiment = pca.fit_transform(firm_sentiment_std)\n",
    "print('Explained Variance Ratios per Component are:\\n {}'\\\n",
    "      .format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_1 = pd.DataFrame(pca.components_.T * \n",
    "                          np.sqrt(pca.explained_variance_), \n",
    "                          columns=['PC1', 'PC2', 'PC3',\n",
    "                                   'PC4', 'PC5', 'PC6'],\n",
    "                          index=firm_sentiment.columns)\n",
    "loadings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loading1 = pd.DataFrame(loadings_1.mean(axis=1))\n",
    "df_loading1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_sentiment = pd.DataFrame(np.dot(pca_market_sentiment,\n",
    "                                     np.array(df_loading1)))\n",
    "merge_ret_bs2['firm_sent'] = firm_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panel Data Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2['log_size'] = np.log(merge_ret_bs2['at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ret_bs2.set_index(['TICKER', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (merge_ret_bs2[['log_size', 'rect', 'ppegt', 'dturn',\n",
    "                'ncskew', 'residuals', 'RoA', 'annual_std',\n",
    "                'firm_sent']]).shift(1)\n",
    "X['neg_outliers'] = merge_ret_bs2['neg_outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyeconometrics.panel_discrete_models import FixedEffectPanelModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FE_ML = FixedEffectPanelModel()\n",
    "FE_ML.fit(X, 'neg_outliers')\n",
    "FE_ML.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X['neg_outliers']\n",
    "X['crash_risk'] = merge_ret_bs2['crash_risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FE_crash = FixedEffectPanelModel()\n",
    "FE_crash.fit(X, 'crash_risk')\n",
    "FE_crash.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
